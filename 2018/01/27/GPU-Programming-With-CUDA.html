<!DOCTYPE html>
<html>
<head>
	<meta charset='UTF-8'>
	<title>GPU Programming With CUDA | Wenwei's tech Blog</title>
	<link rel='alternate' type='application/rss+xml' title='Wenwei Feed' href='http://weweng.github.io/atom.xml'>
	<meta name='description' content=''/>
	<meta name='keywords' content='GPU CUDA parallel computing'/>
	<meta name='viewport' content='width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no'>
	<meta name='theme-color' content='#655B54'>
	<link rel='shortcut icon' href='/assets/favicon.ico' type='image/x-icon'> 
	<!--- <link rel='icon' href='/assets/favicon.ico' type='image/x-icon'> --->
<link rel='icon' href='/assets/blogger.png' type='image/x-png'>

	<link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700' rel='stylesheet' type='text/css'>
	<link rel='stylesheet' type='text/css' href='/assets/css/code-highlighter.css' />
<link rel='stylesheet' type='text/css' href='/assets/css/default.min.css' />
<link rel='stylesheet' type='text/css' href='/assets/css/github.css' />
<link rel='stylesheet' type='text/css' href='/assets/css/pure-base-min.css' />
<link rel='stylesheet' type='text/css' href='/assets/css/pure-grids-responsive-min.css' />
<link rel='stylesheet' type='text/css' href='/assets/css/pure-menus-min.css' />
<link rel='stylesheet' type='text/css' href='/assets/css/pure-tables-min.css' />
<link rel='stylesheet' type='text/css' href='/assets/css/social-buttons.css' />
<link rel='stylesheet' type='text/css' href='/assets/css/weweng.css' />


	
</head>
<body>

<div class='pure-g' id='layout'>
    <div class='sidebar pure-u-1 pure-u-md-1-4'>
      <div class='header'>
	<nav class='nav'>
	  <ul class='nav-list'>
	    <li class='nav-item'>
	      <strong> <a href='/workblog.html'>Blog</a> </strong>
	    </li>
<!--
	    <li class='nav-item'>
	      <strong> <a href='/study/index.html'>Study</a> </strong>
	    </li>
	    <li class='nav-item'>
	      <strong> <a href='/projects/index.html'>Projects</a> </strong>
	    </li>
-->
<!--
            <li class='nav-item'>
              <strong> <a href='/services/index.html'>读书笔记</a> </strong>
            </li>
-->
	    <li class='nav-item'>
	      <strong><a href='/aboutme.html'>About Me</a></strong>
	    </li>
	</ul>
	</nav>
	<br> 
	<h1 class='brand-title'>
	  <a href='/index.html'>
		<img src='/assets/img/avatar.png' height='100'/>
	  </a>
	</h1>
      </div>
</div>

<div class='pure-u-1'>
	<div class='content'>
		<section class='post'>
	<header class='post-header'>
		<h2 class='post-title'> GPU Programming With CUDA </h2>
		<p class='post-meta'>
			Posted on <time class='date' datetime='2018-01-27T00:00:00+01:00'>27 Jan 2018</time>
		</p>
	</header>

	<div class='post-description'>
		<p>Parallel computing using general purpose GPU is really taking off with advancement of technology from Nvidia, AMD, Intel. Especially Nvidia is dominating the field with variety of GPU offering and also software infrastructure CUDA (initially called as Compute Unified Device Architecture), which is a parallel computing platform and application programming interface (API) model. In this article, I share how GPU programming with CUDA looks like using UCS server with Nvidia GPU GRID K1</p>

<p><img src="/uploads/linux/gpu-programming-cuda.jpg" alt="GPU Programming with CUDA" /></p>

<!--more-->
<h2 id="gpu-programming-model">GPU Programming model</h2>

<p>First introducing two terminologies:</p>

<ul>
  <li>Host: The CPU (e.g. x86, ARM) and its memory (host memory)</li>
  <li>Device: The GPU (e.g. Nvidia GPU) and its memory (device memory).</li>
</ul>

<p>There will be host code, which is executed in host CPU (e.g. x86); and there will be device code, which is loaded in host and push into GPU to run. The following diagram shows programming model and execution flow.</p>

<p><img src="/uploads/linux/gpu-programming-cuda-model.jpg" alt="GPU Programming with CUDA model" /></p>

<p>The basic hello world code is shown below:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash"> <span class="nb">cat </span>hello.cu

__global__ void mykernel<span class="o">(</span>void<span class="o">)</span> <span class="o">{</span>

<span class="o">}</span>

int main<span class="o">(</span>void<span class="o">)</span> <span class="o">{</span>
    mykernel<span class="o">&lt;&lt;&lt;</span>1,1&gt;&gt;&gt;<span class="o">()</span><span class="p">;</span>
    <span class="nb">printf</span><span class="o">(</span><span class="s2">"Hello World!</span><span class="se">\n</span><span class="s2">"</span><span class="o">)</span><span class="p">;</span>
    <span class="k">return </span>0<span class="p">;</span>
<span class="o">}</span></code></pre></figure>

<p>The host code is same as usual, but the device code is marked with a new keyword “<strong>global</strong>”.
host code invokes device code almost same as usual, except it adds «1,1» which means using one block and one thread, which isn’t interesting at all. Let’s trying something more interesting.</p>

<h2 id="running-an-example-of-vector-additions-using-mutiple-block-and-multiple-threads">Running an example of vector additions using mutiple block and multiple threads</h2>

<p>The main advantage of GPU computing is to have huge numbers of parallel executions. For that, CUDA introduces:</p>
<ul>
  <li>Block: On the device, each block can execute in parallel, each block has index of “blockIdx.x”</li>
  <li>Thread: a block can be split into parallel threads, each thread has index of “threadIdx.x”</li>
  <li>Combining block with thread: the index is “threadIdx.x + blockIdx.x * blockDim.x”</li>
</ul>

<p><img src="/uploads/linux/gpu-programming-block-thread.jpg" alt="GPU block thread index access" /></p>

<p>The below code creates two input arrays, which holds random integers, and the third array to hold result of addition, which is to be done by GPU.</p>

<p>First make sure we have Nvidia GPU GRID K1 and Nvidia compiler in place:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-1:~/cuda-ex<span class="nv">$ </span>nvidia-smi 
Fri Jun  2 06:41:22 2017       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 367.124                Driver Version: 367.124                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|<span class="o">===============================</span>+<span class="o">======================</span>+<span class="o">======================</span>|
|   0  GRID K1             Off  | 0000:85:00.0     Off |                  N/A |
| N/A   34C    P0    13W /  31W |      0MiB /  4036MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   1  GRID K1             Off  | 0000:86:00.0     Off |                  N/A |
| N/A   35C    P0    13W /  31W |      0MiB /  4036MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   2  GRID K1             Off  | 0000:87:00.0     Off |                  N/A |
| N/A   27C    P0    13W /  31W |      0MiB /  4036MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
|   3  GRID K1             Off  | 0000:88:00.0     Off |                  N/A |
| N/A   30C    P0    12W /  31W |      0MiB /  4036MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID  Type  Process name                               Usage      |
|<span class="o">=============================================================================</span>|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
iot@iotg-ml-1:~/cuda-ex<span class="err">$</span>
iot@iotg-ml-1:~/cuda-ex<span class="nv">$ </span>which nvcc
/usr/local/cuda/bin/nvcc
iot@iotg-ml-1:~/cuda-ex<span class="nv">$ </span>nvcc <span class="nt">--version</span>
nvcc: NVIDIA <span class="o">(</span>R<span class="o">)</span> Cuda compiler driver
Copyright <span class="o">(</span>c<span class="o">)</span> 2005-2016 NVIDIA Corporation
Built on Sun_Sep__4_22:14:01_CDT_2016
Cuda compilation tools, release 8.0, V8.0.44
iot@iotg-ml-1:~/cuda-ex<span class="nv">$ </span></code></pre></figure>

<p>Now let’s try following example:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-1:~/cuda-ex<span class="nv">$ </span><span class="nb">cat </span>gpu-add-vector.cu 
<span class="c">#include &lt;stdio.h&gt;</span>

<span class="c">#define N (2048*2048)</span>
<span class="c">#define THREADS_PER_BLOCK 512</span>

__global__ void add<span class="o">(</span>int <span class="k">*</span>a, int <span class="k">*</span>b, int <span class="k">*</span>c, int n<span class="o">)</span> <span class="o">{</span>
	int index <span class="o">=</span> threadIdx.x + blockIdx.x <span class="k">*</span> blockDim.x<span class="p">;</span>
	<span class="k">if</span> <span class="o">(</span>index &lt; n<span class="o">)</span>
		c[index] <span class="o">=</span> a[index] + b[index]<span class="p">;</span>
<span class="o">}</span>

int main<span class="o">(</span>void<span class="o">)</span> <span class="o">{</span>
	int <span class="k">*</span>a, <span class="k">*</span>b, <span class="k">*</span>c<span class="p">;</span>	// host copies of a, b, c
	int <span class="k">*</span>d_a, <span class="k">*</span>d_b, <span class="k">*</span>d_c<span class="p">;</span> // device copies of a, b, c
	int size <span class="o">=</span> N <span class="k">*</span> sizeof<span class="o">(</span>int<span class="o">)</span><span class="p">;</span>
	int i<span class="p">;</span>

	// Alloc space <span class="k">for </span>device copies of a, b, c
	cudaMalloc<span class="o">((</span>void <span class="k">**</span><span class="o">)</span>&amp;d_a, size<span class="o">)</span><span class="p">;</span>
	cudaMalloc<span class="o">((</span>void <span class="k">**</span><span class="o">)</span>&amp;d_b, size<span class="o">)</span><span class="p">;</span>
	cudaMalloc<span class="o">((</span>void <span class="k">**</span><span class="o">)</span>&amp;d_c, size<span class="o">)</span><span class="p">;</span>
	// Alloc space <span class="k">for </span>host copies of a, b, c and setup input values
	a <span class="o">=</span> <span class="o">(</span>int <span class="k">*</span><span class="o">)</span>malloc<span class="o">(</span>size<span class="o">)</span><span class="p">;</span>  <span class="k">for</span> <span class="o">(</span>i <span class="o">=</span> 0<span class="p">;</span> i &lt; N<span class="p">;</span> i++<span class="o">)</span>  a[i] <span class="o">=</span> rand<span class="o">()</span>/10<span class="p">;</span>
	b <span class="o">=</span> <span class="o">(</span>int <span class="k">*</span><span class="o">)</span>malloc<span class="o">(</span>size<span class="o">)</span><span class="p">;</span>  <span class="k">for</span> <span class="o">(</span>i <span class="o">=</span> 0<span class="p">;</span> i &lt; N<span class="p">;</span> i++<span class="o">)</span>  b[i] <span class="o">=</span> rand<span class="o">()</span>/10<span class="p">;</span>
	c <span class="o">=</span> <span class="o">(</span>int <span class="k">*</span><span class="o">)</span>malloc<span class="o">(</span>size<span class="o">)</span><span class="p">;</span>
	
	// Copy inputs to device
	cudaMemcpy<span class="o">(</span>d_a, a, size, cudaMemcpyHostToDevice<span class="o">)</span><span class="p">;</span>
	cudaMemcpy<span class="o">(</span>d_b, b, size, cudaMemcpyHostToDevice<span class="o">)</span><span class="p">;</span>
	
	// Launch add<span class="o">()</span> kernel on GPU
	add<span class="o">&lt;&lt;&lt;</span>N/THREADS_PER_BLOCK,THREADS_PER_BLOCK&gt;&gt;&gt;<span class="o">(</span>d_a, d_b, d_c, N<span class="o">)</span><span class="p">;</span>
	
	// Copy result back to host
	cudaMemcpy<span class="o">(</span>c, d_c, size, cudaMemcpyDeviceToHost<span class="o">)</span><span class="p">;</span>
	
	// Verify the result
	<span class="nb">printf</span><span class="o">(</span><span class="s2">"</span><span class="se">\n</span><span class="s2"> Verifying the result:"</span><span class="o">)</span><span class="p">;</span>
        <span class="k">for</span> <span class="o">(</span>i <span class="o">=</span> 0<span class="p">;</span> i &lt; N<span class="p">;</span> i++<span class="o">)</span> <span class="o">{</span>
	  <span class="k">if</span> <span class="o">((</span>a[i] + b[i]<span class="o">)</span> <span class="o">!=</span> c[i]<span class="o">)</span> <span class="o">{</span>
	    <span class="nb">printf</span><span class="o">(</span><span class="s2">"Failed at %d: a=%d, b=%d, c=%d </span><span class="se">\n</span><span class="s2">"</span>, i, a[i], b[i], c[i]<span class="o">)</span><span class="p">;</span>
	    <span class="nb">break</span><span class="p">;</span>
	  <span class="o">}</span>
	<span class="o">}</span>
        <span class="k">if</span> <span class="o">(</span>i <span class="o">==</span> N<span class="o">)</span> <span class="nb">printf</span><span class="o">(</span><span class="s2">"PASSED!</span><span class="se">\n\n</span><span class="s2">"</span><span class="o">)</span><span class="p">;</span>

	// Cleanup
	free<span class="o">(</span>a<span class="o">)</span><span class="p">;</span> free<span class="o">(</span>b<span class="o">)</span><span class="p">;</span> free<span class="o">(</span>c<span class="o">)</span><span class="p">;</span>
	cudaFree<span class="o">(</span>d_a<span class="o">)</span><span class="p">;</span> cudaFree<span class="o">(</span>d_b<span class="o">)</span><span class="p">;</span> cudaFree<span class="o">(</span>d_c<span class="o">)</span><span class="p">;</span>
	
	<span class="k">return</span> <span class="o">(</span>0<span class="o">)</span><span class="p">;</span>
<span class="o">}</span>

iot@iotg-ml-1:~/cuda-ex<span class="nv">$ </span>nvcc gpu-add-vector.cu <span class="nt">-o</span> gpu-add-vector
nvcc warning : The <span class="s1">'compute_20'</span>, <span class="s1">'sm_20'</span>, and <span class="s1">'sm_21'</span> architectures are deprecated, and may be removed <span class="k">in </span>a future release <span class="o">(</span>Use <span class="nt">-Wno-deprecated-gpu-targets</span> to suppress warning<span class="o">)</span><span class="nb">.</span>
iot@iotg-ml-1:~/cuda-ex<span class="nv">$ </span>./gpu-add-vector 

 Verifying the result:PASSED!
 
iot@iotg-ml-1:~/cuda-ex<span class="nv">$ </span></code></pre></figure>

<h1 id="reference">Reference:</h1>
<ul>
  <li>https://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf</li>
  <li>https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html</li>
</ul>


	</div>

<!--	
	<div class='share-button-container'>
		<p>Like it? Share it!</p>
		<div class='share-button share-button-twitter' data-share-url="http://weweng.github.io/2018/01/27/GPU-Programming-With-CUDA.html">
			<div clas='box'>
				<a href="https://twitter.com/share?url=http://weweng.github.io%2F2018%2F01%2F27%2FGPU-Programming-With-CUDA.html&text=GPU Programming With CUDA&via=weweng" target='_blank'>
					<span class='share'>Tweet</span>
					<span class='count'>0</span>
				</a>
			</div>
		</div>

		<div class="share-button share-button-facebook" data-share-url="http://weweng.github.io/2018/01/27/GPU-Programming-With-CUDA.html">
			<div class="box">
				<a href="https://www.facebook.com/sharer/sharer.php?u=http://weweng.github.io%2F2018%2F01%2F27%2FGPU-Programming-With-CUDA.html">
					<span class='share'>Share</span>
					<span class='count'>0</span>
				</a>
			</div>
		</div>
	</div>
-->

</section>

<div id='comments'>
	<div id='disqus_thread'></div>
	<script type='text/javascript'>
		var disqus_shortname = 'wewenggithubio';
		(function() {
			var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
			dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
			(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
		})();
	</script>

	<noscript>Please enable JavaScript to view the comments</noscript>
</div>
<script type='text/javascript' src='/assets/js/StaticShareButtons.js'></script>


	</div>

	<footer class='footer'>
		<span>Copyright 2015-2017, Wenwei Weng</span>
	</footer>
</div>
</div>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-62997612-1', 'auto');
  ga('send', 'pageview');

</script>


<script type="text/javascript">
// Open external links in new tab
(window.onload = function(){
	var links = document.links;

	for(i=0; i<=links.length -1; i++){
		if(links[i].hostname != window.location.hostname){
			links[i].target = '_blank';
		}
	}
})();
</script>
</body>
</html>
