<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Building a Machine Learning platform using UCS 220 M5 with Tesla P4 GPU</title>
  <meta name="description" content="Machine Learning has been a very hot field. Big players like Amazon, Google, they host machine learning infrastructure in the cloud, which is very nice thing...">
  
  <meta name="author" content="Wenwei Weng">
  <meta name="copyright" content="&copy; Wenwei Weng 2023">
  

  <!-- External libraries -->
  <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/styles/monokai-sublime.min.css">
  <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.7.1/css/lightbox.css">

  <!-- Favicon and other icons (made with http://www.favicon-generator.org/) -->
  <link rel="shortcut icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="icon" href="/assets/icons/favicon.ico" type="image/x-icon">
  <link rel="apple-touch-icon" sizes="57x57" href="/assets/icons/apple-icon-57x57.png">
  <link rel="apple-touch-icon" sizes="60x60" href="/assets/icons/apple-icon-60x60.png">
  <link rel="apple-touch-icon" sizes="72x72" href="/assets/icons/apple-icon-72x72.png">
  <link rel="apple-touch-icon" sizes="76x76" href="/assets/icons/apple-icon-76x76.png">
  <link rel="apple-touch-icon" sizes="114x114" href="/assets/icons/apple-icon-114x114.png">
  <link rel="apple-touch-icon" sizes="120x120" href="/assets/icons/apple-icon-120x120.png">
  <link rel="apple-touch-icon" sizes="144x144" href="/assets/icons/apple-icon-144x144.png">
  <link rel="apple-touch-icon" sizes="152x152" href="/assets/icons/apple-icon-152x152.png">
  <link rel="apple-touch-icon" sizes="180x180" href="/assets/icons/apple-icon-180x180.png">
  <link rel="icon" type="image/png" sizes="192x192"  href="/assets/icons/android-icon-192x192.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/assets/icons/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="96x96" href="/assets/icons/favicon-96x96.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/assets/icons/favicon-16x16.png">
  <link rel="manifest" href="/assets/icons/manifest.json">
  <meta name="msapplication-TileColor" content="#ffffff">
  <meta name="msapplication-TileImage" content="/assets/icons/ms-icon-144x144.png">
  <meta name="theme-color" content="#ffffff">

  

  

  

  <!-- Site styles -->
  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/linux/2018/09/08/UCS220-M5-ML-platform.html">
	<link rel="alternate" type="application/rss+xml" title="Wenwei Weng's Blog" href="http://localhost:4000/feed.xml" />
	
	<!-- Tooltips -->
	<script type="text/javascript">
		window.tooltips = []
	</script>
</head>


  <body>

    <header class="navigation" role="banner">
  <div class="navigation-wrapper">
    <a href="/" class="logo">
      
      <img src="/assets/avatar.png" alt="Wenwei Weng's Blog">
      
    </a>
    <a href="javascript:void(0)" class="navigation-menu-button" id="js-mobile-menu">
      <i class="fa fa-bars"></i>
    </a>
    <nav role="navigation">
      <ul id="js-navigation-menu" class="navigation-menu show">
				
	
	<li class="nav-link"><a href="/about/">About</a>
	

	

	

	
	<li class="nav-link"><a href="/posts/">Posts</a>
	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	


      </ul>
    </nav>
  </div>
</header>


    <div class="page-content">
        <div class="post">

<div class="post-header-container " >
  <div class="scrim ">
    <header class="post-header">
      <h1 class="title">Building a Machine Learning platform using UCS 220 M5 with Tesla P4 GPU</h1>
      <p class="info">by <strong>Wenwei Weng</strong></p>
    </header>
  </div>
</div>

<div class="wrapper">

 <span class="page-divider">
  <span class="one"></span>
  <span class="two"></span>
</span>
 

<section class="post-meta">
  <div class="post-date">September 8, 2018</div>
  <div class="post-categories">
  in 
    
    <a href="/category/Linux">Linux</a>
    
  
  </div>
</section>

<article class="post-content">
  <p>Machine Learning has been a very hot field. Big players like Amazon, Google, they host machine learning infrastructure in the cloud, which is very nice thing, except it can be very expensive to use it. Iâ€™m always curious if I can build one by myself: a relative powerful machine learning platform. The following shows what I achieved using UCS220-M5 with Tesla-P4 GPU.</p>

<p><img src="/uploads/ucs/c220-m5-tesla-p4-ml.jpg" alt="Required Hardware" /> 
<!--more--></p>
<h2 id="what-is-required-hardware">What is required hardware?</h2>
<ul>
  <li>CPU: Dual Intel Xeon 6132 @2.6 GHz, 28 cores/each</li>
  <li>GPU: Nvidia Tesla P4 (Compute Capability: 6.1)</li>
  <li>DDR4: 192 GB</li>
  <li>HDD: 2TB</li>
</ul>

<p>Cisco UCS220-M5 is a good platform to start with!</p>

<h2 id="what-is-software-stack-to-be-built">What is software stack to be built?</h2>
<p>The following diagram shows the typical ML software stack. In my case, I build software stack from bottom-up, however top layer, I only build TensorFlow.</p>

<p><img src="/uploads/ucs/ml-sw-stack.jpg" alt="ML software stack" /></p>

<h4 id="step1-get-ubuntu-1604-box-ready">Step#1 Get ubuntu 16.04 box ready</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~<span class="nv">$ </span>lsb_release <span class="nt">-a</span>
No LSB modules are available.
Distributor ID:	Ubuntu
Description:	Ubuntu 16.04.5 LTS
Release:	16.04
Codename:	xenial
iot@iotg-ml-2:~<span class="nv">$ </span>lspci | <span class="nb">grep</span> <span class="nt">-i</span> nvidia
d8:00.0 3D controller: NVIDIA Corporation Device 1bb3 <span class="o">(</span>rev a1<span class="o">)</span>
iot@iotg-ml-2:~<span class="nv">$ </span>
iot@iotg-ml-2:~<span class="nv">$ </span><span class="nb">sudo </span>apt-get update
<span class="o">[</span><span class="nb">sudo</span><span class="o">]</span> password <span class="k">for </span>iot: 
Hit:1 http://us.archive.ubuntu.com/ubuntu xenial InRelease
Hit:2 http://us.archive.ubuntu.com/ubuntu xenial-updates InRelease      
Hit:3 http://us.archive.ubuntu.com/ubuntu xenial-backports InRelease    
Hit:4 http://security.ubuntu.com/ubuntu xenial-security InRelease
Reading package lists... Done                     
iot@iotg-ml-2:~<span class="nv">$ </span><span class="nb">sudo </span>apt-get <span class="nb">install </span>build-essential linux-headers-<span class="si">$(</span><span class="nb">uname</span> <span class="nt">-r</span><span class="si">)</span>
Reading package lists... Done
Building dependency tree       
Reading state information... Done
build-essential is already the newest version <span class="o">(</span>12.1ubuntu2<span class="o">)</span><span class="nb">.</span>
linux-headers-4.15.0-29-generic is already the newest version <span class="o">(</span>4.15.0-29.31~16.04.1<span class="o">)</span><span class="nb">.</span>
linux-headers-4.15.0-29-generic <span class="nb">set </span>to manually installed.
0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
iot@iotg-ml-2:~<span class="nv">$ </span>gcc <span class="nt">--version</span>
gcc <span class="o">(</span>Ubuntu 5.4.0-6ubuntu1~16.04.10<span class="o">)</span> 5.4.0 20160609
Copyright <span class="o">(</span>C<span class="o">)</span> 2015 Free Software Foundation, Inc.
This is free software<span class="p">;</span> see the <span class="nb">source </span><span class="k">for </span>copying conditions.  There is NO
warranty<span class="p">;</span> not even <span class="k">for </span>MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.

iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo </span>apt-get <span class="nb">install </span>libncurses5-dev libncursesw5-dev
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  libtinfo-dev
Suggested packages:
  ncurses-doc
The following NEW packages will be installed:
  libncurses5-dev libncursesw5-dev libtinfo-dev
0 upgraded, 3 newly installed, 0 to remove and 39 not upgraded.
Need to get 450 kB of archives.
After this operation, 2,642 kB of additional disk space will be used.
Do you want to <span class="k">continue</span>? <span class="o">[</span>Y/n] y
Get:1 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libtinfo-dev amd64 6.0+20160213-1ubuntu1 <span class="o">[</span>77.4 kB]
Get:2 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libncurses5-dev amd64 6.0+20160213-1ubuntu1 <span class="o">[</span>175 kB]
Get:3 http://us.archive.ubuntu.com/ubuntu xenial/main amd64 libncursesw5-dev amd64 6.0+20160213-1ubuntu1 <span class="o">[</span>198 kB]
Fetched 450 kB <span class="k">in </span>3s <span class="o">(</span>140 kB/s<span class="o">)</span>            
Selecting previously unselected package libtinfo-dev:amd64.
<span class="o">(</span>Reading database ... 220106 files and directories currently installed.<span class="o">)</span>
Preparing to unpack .../libtinfo-dev_6.0+20160213-1ubuntu1_amd64.deb ...
Unpacking libtinfo-dev:amd64 <span class="o">(</span>6.0+20160213-1ubuntu1<span class="o">)</span> ...
Selecting previously unselected package libncurses5-dev:amd64.
Preparing to unpack .../libncurses5-dev_6.0+20160213-1ubuntu1_amd64.deb ...
Unpacking libncurses5-dev:amd64 <span class="o">(</span>6.0+20160213-1ubuntu1<span class="o">)</span> ...
Selecting previously unselected package libncursesw5-dev:amd64.
Preparing to unpack .../libncursesw5-dev_6.0+20160213-1ubuntu1_amd64.deb ...
Unpacking libncursesw5-dev:amd64 <span class="o">(</span>6.0+20160213-1ubuntu1<span class="o">)</span> ...
Processing triggers <span class="k">for </span>man-db <span class="o">(</span>2.7.5-1<span class="o">)</span> ...
Setting up libtinfo-dev:amd64 <span class="o">(</span>6.0+20160213-1ubuntu1<span class="o">)</span> ...
Setting up libncurses5-dev:amd64 <span class="o">(</span>6.0+20160213-1ubuntu1<span class="o">)</span> ...
Setting up libncursesw5-dev:amd64 <span class="o">(</span>6.0+20160213-1ubuntu1<span class="o">)</span> ...

iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo </span>apt-get <span class="nb">install </span>libelf-dev
...
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span> 
iot@iotg-ml-2:~<span class="nv">$ </span><span class="nb">uname</span> <span class="nt">-a</span>
Linux iotg-ml-2 4.15.0-29-generic <span class="c">#31~16.04.1-Ubuntu SMP Wed Jul 18 08:54:04 UTC 2018 x86_64 x86_64 x86_64 GNU/Linux</span>
iot@iotg-ml-2:~<span class="nv">$ </span> 
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo </span>apt-get <span class="nt">--purge</span> remove nvidia-<span class="k">*</span>
Reading package lists... Done
Building dependency tree       
Reading state information... Done
Note, selecting <span class="s1">'nvidia-325-updates'</span> <span class="k">for </span>glob <span class="s1">'nvidia-*'</span>
....
Package <span class="s1">'nvidia-opencl-icd-375'</span> is not installed, so not removed
0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span></code></pre></figure>

<h4 id="step2-install-nvidia-driver-for-tesla-p4">Step#2 Install Nvidia Driver for Tesla P4</h4>
<p>From Nvidia site, Tesla P4 driver: 
https://www.nvidia.com/download/driverResults.aspx/112428/en-us
TESLA DRIVER FOR LINUX X64</p>

<p>Version:	375.20
Release Date:	2016.12.9
Operating System:	Linux 64-bit
CUDA Toolkit:	8.0
Language:	English (India)
File Size:	72.37 MB
New in Release 375.20
New Device Support
P100
P4</p>

<p>Recommended CUDA version(s):
CUDA 8.0</p>

<p>iot@iotg-ml-2:~/Downloads$ ls -l NVIDIA-Linux-x86_64-375.20.run
-rw-rw-râ€“ 1 iot iot 75886564 Nov 16  2016 NVIDIA-Linux-x86_64-375.20.run</p>

<h5 id="disable-x-display-server">Disable X display server:</h5>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo rm</span> /etc/X11/xorg.conf
<span class="nb">rm</span>: cannot remove <span class="s1">'/etc/X11/xorg.conf'</span>: No such file or directory
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span>
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">cat</span> /etc/modprobe.d/blacklist-nouveau.conf
blacklist nouveau
options nouveau <span class="nv">modeset</span><span class="o">=</span>0
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo </span>update-initramfs <span class="nt">-u</span>
update-initramfs: Generating /boot/initrd.img-4.15.0-34-generic
Reboot into text mode <span class="o">(</span>runlevel 3<span class="o">)</span><span class="nb">.</span>
This can usually be accomplished by adding the number <span class="s2">"3"</span> to the end of the system<span class="s1">'s kernel boot parameters.
iot@iotg-ml-2:~/Downloads$ sudo reboot</span></code></pre></figure>

<h5 id="install-driver-from-downloaded-run-file-failed">Install driver from downloaded run file: failed!!!</h5>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~<span class="nv">$ </span><span class="nb">sudo </span>service lightdm stop
<span class="o">[</span><span class="nb">sudo</span><span class="o">]</span> password <span class="k">for </span>iot: 
iot@iotg-ml-2:~<span class="nv">$ </span><span class="nb">cd</span> ~/Downloads/
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span>
total 1600552
<span class="nt">-rwxr-xr-x</span> 1 iot iot   97546170 Sep 26 22:44 cuda_8.0.61.2_linux.run
<span class="nt">-rwxr-xr-x</span> 1 iot iot 1465528129 Sep 26 22:45 cuda_8.0.61_375.26_linux.run
<span class="nt">-rw-rw-r--</span> 1 iot iot   75886564 Nov 16  2016 NVIDIA-Linux-x86_64-375.20.run
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">chmod</span> +x NVIDIA-Linux-x86_64-375.20.run 
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo</span> ./NVIDIA-Linux-x86_64-375.20.run <span class="nt">--no-opengl-files</span>
Verifying archive integrity... OK
Uncompressing NVIDIA Accelerated Graphics Driver <span class="k">for </span>Linux-x86_64 375.20.....
<span class="o">(</span>
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">cat</span> /usr/lib/nvidia/pre-install
<span class="c">#!/bin/sh</span>
<span class="c"># Trigger an error exit status to prevent the installer from overwriting</span>
<span class="c"># Ubuntu's nvidia packages.</span>
<span class="nb">exit </span>1
iot@iotg-ml-2:~/Downloads<span class="err">$</span></code></pre></figure>

<p>local run file installation failed because this 16.04.5 is latest version with kernel 4.15.0, which is not compatible with installer file. Switch to use ubuntu repo version.</p>

<h5 id="install-driver-using-apt-get-luckily-it-succeeds">Install driver using apt-get, luckily, it succeeds!</h5>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo </span>apt-get <span class="nb">install </span>nvidia-375 nvidia-modprobe
...
nvidia_384:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/4.15.0-34-generic/updates/dkms/
nvidia_384_modeset.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/4.15.0-34-generic/updates/dkms/
 
nvidia_384_drm.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/4.15.0-34-generic/updates/dkms/
 
nvidia_384_uvm.ko:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/4.15.0-34-generic/updates/dkms/
depmod............
DKMS: <span class="nb">install </span>completed.
Setting up libcuda1-384 <span class="o">(</span>384.130-0ubuntu0.16.04.1<span class="o">)</span> ...
Setting up libjansson4:amd64 <span class="o">(</span>2.7-3ubuntu0.1<span class="o">)</span> ...
Setting up libvdpau1:amd64 <span class="o">(</span>1.1.1-3ubuntu1<span class="o">)</span> ...
Setting up libxnvctrl0 <span class="o">(</span>361.42-0ubuntu1<span class="o">)</span> ...
Setting up mesa-vdpau-drivers:amd64 <span class="o">(</span>18.0.5-0ubuntu0~16.04.1<span class="o">)</span> ...
Setting up nvidia-375 <span class="o">(</span>384.130-0ubuntu0.16.04.1<span class="o">)</span> ...
Setting up nvidia-modprobe <span class="o">(</span>361.28-1<span class="o">)</span> ...
Setting up ocl-icd-libopencl1:amd64 <span class="o">(</span>2.2.8-1<span class="o">)</span> ...
Setting up nvidia-opencl-icd-384 <span class="o">(</span>384.130-0ubuntu0.16.04.1<span class="o">)</span> ...
Setting up bbswitch-dkms <span class="o">(</span>0.8-3ubuntu1<span class="o">)</span> ...
Loading new bbswitch-0.8 DKMS files...
First Installation: checking all kernels...
Building only <span class="k">for </span>4.15.0-34-generic
Building initial module <span class="k">for </span>4.15.0-34-generic
Done.
bbswitch:
Running module version sanity check.
 - Original module
   - No original module exists within this kernel
 - Installation
   - Installing to /lib/modules/4.15.0-34-generic/updates/dkms/
depmod....
DKMS: <span class="nb">install </span>completed.
Setting up nvidia-prime <span class="o">(</span>0.8.2<span class="o">)</span> ...
Setting up screen-resolution-extra <span class="o">(</span>0.17.1.1~16.04.1<span class="o">)</span> ...
Setting up nvidia-settings <span class="o">(</span>361.42-0ubuntu1<span class="o">)</span> ...
Setting up vdpau-driver-all:amd64 <span class="o">(</span>1.1.1-3ubuntu1<span class="o">)</span> ...
Processing triggers <span class="k">for </span>libc-bin <span class="o">(</span>2.23-0ubuntu10<span class="o">)</span> ...
Processing triggers <span class="k">for </span>initramfs-tools <span class="o">(</span>0.122ubuntu8.11<span class="o">)</span> ...
update-initramfs: Generating /boot/initrd.img-4.15.0-34-generic
Processing triggers <span class="k">for </span>shim-signed <span class="o">(</span>1.33.1~16.04.1+13-0ubuntu2<span class="o">)</span> ...
Secure Boot not enabled on this system.
Processing triggers <span class="k">for </span>ureadahead <span class="o">(</span>0.100.0-19<span class="o">)</span> ...
Processing triggers <span class="k">for </span>dbus <span class="o">(</span>1.10.6-1ubuntu3.3<span class="o">)</span> ...
iot@iotg-ml-2:~/Downloads<span class="err">$</span></code></pre></figure>

<p>Wow, it passed! Now letâ€™s check if we can see Tesla-P4 device properly?</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~/Downloads<span class="nv">$ </span>nvidia-smi 
Thu Sep 27 11:30:37 2018       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 384.130                Driver Version: 384.130                   |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|<span class="o">===============================</span>+<span class="o">======================</span>+<span class="o">======================</span>|
|   0  Tesla P4            Off  | 00000000:D8:00.0 Off |                    0 |
| N/A   37C    P0    23W /  75W |      0MiB /  7606MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
 
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|<span class="o">=============================================================================</span>|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span></code></pre></figure>

<p>Very nice! we can see Tesla-P4.</p>

<p>As it turns out version 384.130 is installed, it means we can use cuda version 9.0
Based on: https://stackoverflow.com/questions/30820513/what-is-the-correct-version-of-cuda-for-my-nvidia-driver/30820690#30820690
CUDA 9.2:396.xx
CUDA 9.1:387.xx
CUDA 9.0:384.xx
CUDA 8.0375.xx(GA2)
CUDA 8.0:367.4x
CUDA 7.5:352.xx Â </p>

<p>After driver is installed, I rebooted the box, and X window display is still working!</p>

<h4 id="step3-install-cuda-version-90">Step#3 Install Cuda version 9.0</h4>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo</span> ./cuda_9.0.176_384.81_linux.run <span class="nt">--no-opengl-libs</span>
...
Install the CUDA 9.0 Samples?
<span class="o">(</span>y<span class="o">)</span>es/<span class="o">(</span>n<span class="o">)</span>o/<span class="o">(</span>q<span class="o">)</span>uit: y
 
Enter CUDA Samples Location

 <span class="o">[</span> default is /home/iot <span class="o">]</span>: 
 
Installing the CUDA Toolkit <span class="k">in</span> /usr/local/cuda-9.0 ...
Missing recommended library: libGLU.so
Missing recommended library: libX11.so
Missing recommended library: libXi.so
Missing recommended library: libXmu.so
 
Installing the CUDA Samples <span class="k">in</span> /home/iot ...
Copying samples to /home/iot/NVIDIA_CUDA-9.0_Samples now...
Finished copying samples.
 
<span class="o">===========</span>
<span class="o">=</span> Summary <span class="o">=</span>
<span class="o">===========</span>
 
Driver:   Not Selected
Toolkit:  Installed <span class="k">in</span> /usr/local/cuda-9.0
Samples:  Installed <span class="k">in</span> /home/iot, but missing recommended libraries
 
Please make sure that
 -   PATH includes /usr/local/cuda-9.0/bin
 -   LD_LIBRARY_PATH includes /usr/local/cuda-9.0/lib64, or, add /usr/local/cuda-9.0/lib64 to /etc/ld.so.conf and run ldconfig as root
 
To uninstall the CUDA Toolkit, run the uninstall script <span class="k">in</span> /usr/local/cuda-9.0/bin
 
Please see CUDA_Installation_Guide_Linux.pdf <span class="k">in</span> /usr/local/cuda-9.0/doc/pdf <span class="k">for </span>detailed information on setting up CUDA.
 
<span class="k">***</span>WARNING: Incomplete installation! This installation did not <span class="nb">install </span>the CUDA Driver. A driver of version at least 384.00 is required <span class="k">for </span>CUDA 9.0 functionality to work.
To <span class="nb">install </span>the driver using this installer, run the following <span class="nb">command</span>, replacing &lt;CudaInstaller&gt; with the name of this run file:
    <span class="nb">sudo</span> &lt;CudaInstaller&gt;.run <span class="nt">-silent</span> <span class="nt">-driver</span>
 
Logfile is /tmp/cuda_install_2263.log
iot@iotg-ml-2:~/Downloads<span class="err">$</span></code></pre></figure>

<p>Verify CUDA software is running fine by using the provided sample application device query:</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~/NVIDIA_CUDA-9.0_Samples<span class="nv">$ </span><span class="nb">pwd</span>
/home/iot/NVIDIA_CUDA-9.0_Samples
iot@iotg-ml-2:~/NVIDIA_CUDA-9.0_Samples<span class="nv">$ </span><span class="nb">ls
</span>0_Simple     2_Graphics  4_Finance      6_Advanced       bin     EULA.txt
1_Utilities  3_Imaging   5_Simulations  7_CUDALibraries  common  Makefile
iot@iotg-ml-2:~/NVIDIA_CUDA-9.0_Samples<span class="nv">$ </span>make
...
...
...
iot@iotg-ml-2:~/NVIDIA_CUDA-9.0_Samples/bin/x86_64/linux/release<span class="nv">$ </span>./deviceQuery

./deviceQuery Starting...
 
 CUDA Device Query <span class="o">(</span>Runtime API<span class="o">)</span> version <span class="o">(</span>CUDART static linking<span class="o">)</span>
 
Detected 1 CUDA Capable device<span class="o">(</span>s<span class="o">)</span>
 
Device 0: <span class="s2">"Tesla P4"</span>
  CUDA Driver Version / Runtime Version          9.0 / 9.0
  CUDA Capability Major/Minor version number:    6.1
  Total amount of global memory:                 7606 MBytes <span class="o">(</span>7975862272 bytes<span class="o">)</span>
  <span class="o">(</span>20<span class="o">)</span> Multiprocessors, <span class="o">(</span>128<span class="o">)</span> CUDA Cores/MP:     2560 CUDA Cores
  GPU Max Clock rate:                            1114 MHz <span class="o">(</span>1.11 GHz<span class="o">)</span>
  Memory Clock rate:                             3003 Mhz
  Memory Bus Width:                              256-bit
  L2 Cache Size:                                 2097152 bytes
  Maximum Texture Dimension Size <span class="o">(</span>x,y,z<span class="o">)</span>         <span class="nv">1D</span><span class="o">=(</span>131072<span class="o">)</span>, <span class="nv">2D</span><span class="o">=(</span>131072, 65536<span class="o">)</span>, <span class="nv">3D</span><span class="o">=(</span>16384, 16384, 16384<span class="o">)</span>
  Maximum Layered 1D Texture Size, <span class="o">(</span>num<span class="o">)</span> layers  <span class="nv">1D</span><span class="o">=(</span>32768<span class="o">)</span>, 2048 layers
  Maximum Layered 2D Texture Size, <span class="o">(</span>num<span class="o">)</span> layers  <span class="nv">2D</span><span class="o">=(</span>32768, 32768<span class="o">)</span>, 2048 layers
  Total amount of constant memory:               65536 bytes
  Total amount of shared memory per block:       49152 bytes
  Total number of registers available per block: 65536
  Warp size:                                     32
  Maximum number of threads per multiprocessor:  2048
  Maximum number of threads per block:           1024
  Max dimension size of a thread block <span class="o">(</span>x,y,z<span class="o">)</span>: <span class="o">(</span>1024, 1024, 64<span class="o">)</span>
  Max dimension size of a grid size    <span class="o">(</span>x,y,z<span class="o">)</span>: <span class="o">(</span>2147483647, 65535, 65535<span class="o">)</span>
  Maximum memory pitch:                          2147483647 bytes
  Texture alignment:                             512 bytes
  Concurrent copy and kernel execution:          Yes with 2 copy engine<span class="o">(</span>s<span class="o">)</span>
  Run <span class="nb">time </span>limit on kernels:                     No
  Integrated GPU sharing Host Memory:            No
  Support host page-locked memory mapping:       Yes
  Alignment requirement <span class="k">for </span>Surfaces:            Yes
  Device has ECC support:                        Enabled
  Device supports Unified Addressing <span class="o">(</span>UVA<span class="o">)</span>:      Yes
  Supports Cooperative Kernel Launch:            Yes
  Supports MultiDevice Co-op Kernel Launch:      Yes
  Device PCI Domain ID / Bus ID / location ID:   0 / 216 / 0
  Compute Mode:
     &lt; Default <span class="o">(</span>multiple host threads can use ::cudaSetDevice<span class="o">()</span> with device simultaneously<span class="o">)</span> <span class="o">&gt;</span>
 
deviceQuery, CUDA Driver <span class="o">=</span> CUDART, CUDA Driver Version <span class="o">=</span> 9.0, CUDA Runtime Version <span class="o">=</span> 9.0, NumDevs <span class="o">=</span> 1
Result <span class="o">=</span> PASS
iot@iotg-ml-2:~/NVIDIA_CUDA-9.0_Samples/bin/x86_64/linux/release<span class="nv">$ </span></code></pre></figure>

<p>All looks cool so far!</p>

<h4 id="step4-install-nvidia-cuda-deep-neural-network-library-cudnn-version-714">Step#4 Install NVIDIA CUDAÂ® Deep Neural Network library (cuDNN) version 7.1.4</h4>
<p>Ref: 
https://medium.com/@zhanwenchen/install-cuda-9-2-and-cudnn-7-1-for-tensorflow-pytorch-gpu-on-ubuntu-16-04-1822ab4b2421</p>

<p>https://developer.nvidia.com/rdp/cudnn-archive
Download cuDNN v7.1.4 (May 16, 2018), for CUDA 9.0
cuDNN v7.1.4 Runtime Library for Ubuntu16.04 (Deb)
cuDNN v7.1.4 Developer Library for Ubuntu16.04 (Deb)
cuDNN v7.1.4 Code Samples and User Guide for Ubuntu16.04 (Deb)</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">ls</span> <span class="nt">-l</span>

total 3781372
<span class="nt">-rwxr-xr-x</span> 1 iot iot   97546170 Sep 26 22:44 cuda_8.0.61.2_linux.run
<span class="nt">-rwxr-xr-x</span> 1 iot iot 1465528129 Sep 26 22:45 cuda_8.0.61_375.26_linux.run
<span class="nt">-rwxr-xr-x</span> 1 iot iot  116529155 Sep 27 12:04 cuda_9.0.176.1_linux.run
<span class="nt">-rwxr-xr-x</span> 1 iot iot   72212159 Sep 27 12:04 cuda_9.0.176.2_linux.run
<span class="nt">-rwxr-xr-x</span> 1 iot iot 1643293725 Sep 27 12:04 cuda_9.0.176_384.81_linux.run
<span class="nt">-rwxr-xr-x</span> 1 iot iot   76500577 Sep 27 12:04 cuda_9.0.176.3_linux.run
<span class="nt">-rwxr-xr-x</span> 1 iot iot   77497173 Sep 27 12:04 cuda_9.0.176.4_linux.run
<span class="nt">-rwxr-xr-x</span> 1 iot iot  126202024 Sep 27 15:11 libcudnn7_7.1.4.18-1+cuda9.0_amd64.deb
<span class="nt">-rwxr-xr-x</span> 1 iot iot  116411616 Sep 27 15:11 libcudnn7-dev_7.1.4.18-1+cuda9.0_amd64.deb
<span class="nt">-rwxr-xr-x</span> 1 iot iot    4491360 Sep 27 15:11 libcudnn7-doc_7.1.4.18-1+cuda9.0_amd64.deb
<span class="nt">-rwxrwxr-x</span> 1 iot iot   75886564 Nov 16  2016 NVIDIA-Linux-x86_64-375.20.run
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo </span>dpkg <span class="nt">-i</span> libcudnn7_7.1.4.18-1+cuda9.0_amd64.deb
<span class="o">[</span><span class="nb">sudo</span><span class="o">]</span> password <span class="k">for </span>iot: 
Selecting previously unselected package libcudnn7.
<span class="o">(</span>Reading database ... 221264 files and directories currently installed.<span class="o">)</span>
Preparing to unpack libcudnn7_7.1.4.18-1+cuda9.0_amd64.deb ...
Unpacking libcudnn7 <span class="o">(</span>7.1.4.18-1+cuda9.0<span class="o">)</span> ...
Setting up libcudnn7 <span class="o">(</span>7.1.4.18-1+cuda9.0<span class="o">)</span> ...
Processing triggers <span class="k">for </span>libc-bin <span class="o">(</span>2.23-0ubuntu10<span class="o">)</span> ...
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo </span>dpkg <span class="nt">-i</span> libcudnn7-dev_7.1.4.18-1+cuda9.0_amd64.deb
Selecting previously unselected package libcudnn7-dev.
<span class="o">(</span>Reading database ... 221271 files and directories currently installed.<span class="o">)</span>
Preparing to unpack libcudnn7-dev_7.1.4.18-1+cuda9.0_amd64.deb ...
Unpacking libcudnn7-dev <span class="o">(</span>7.1.4.18-1+cuda9.0<span class="o">)</span> ...
Setting up libcudnn7-dev <span class="o">(</span>7.1.4.18-1+cuda9.0<span class="o">)</span> ...
update-alternatives: using /usr/include/x86_64-linux-gnu/cudnn_v7.h to provide /usr/include/cudnn.h <span class="o">(</span>libcudnn<span class="o">)</span> <span class="k">in </span>auto mode
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo </span>dpkg <span class="nt">-i</span> libcudnn7-doc_7.1.4.18-1+cuda9.0_amd64.deb
Selecting previously unselected package libcudnn7-doc.
<span class="o">(</span>Reading database ... 221277 files and directories currently installed.<span class="o">)</span>
Preparing to unpack libcudnn7-doc_7.1.4.18-1+cuda9.0_amd64.deb ...
Unpacking libcudnn7-doc <span class="o">(</span>7.1.4.18-1+cuda9.0<span class="o">)</span> ...
Setting up libcudnn7-doc <span class="o">(</span>7.1.4.18-1+cuda9.0<span class="o">)</span> ...
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span>
 
After successful installation, we can verify <span class="k">if </span>it is working:
 
iot@iotg-ml-2:~<span class="nv">$ </span><span class="nb">cp</span> <span class="nt">-r</span> /usr/src/cudnn_samples_v7/ <span class="nb">.</span>
iot@iotg-ml-2:~<span class="nv">$ </span><span class="nb">cd </span>cudnn_samples_v7/
iot@iotg-ml-2:~/cudnn_samples_v7<span class="nv">$ </span><span class="nb">ls
</span>conv_sample  mnistCUDNN  RNN
iot@iotg-ml-2:~/cudnn_samples_v7<span class="nv">$ </span><span class="nb">cd </span>mnistCUDNN/
iot@iotg-ml-2:~/cudnn_samples_v7/mnistCUDNN<span class="nv">$ </span><span class="nb">ls
</span>data  error_util.h  fp16_dev.cu  fp16_dev.h  fp16_emu.cpp  fp16_emu.h  FreeImage  gemv.h  Makefile  mnistCUDNN.cpp  readme.txt
iot@iotg-ml-2:~/cudnn_samples_v7/mnistCUDNN<span class="nv">$ </span>make clean <span class="o">&amp;&amp;</span> make
<span class="nb">rm</span> <span class="nt">-rf</span> <span class="k">*</span>o
<span class="nb">rm</span> <span class="nt">-rf</span> mnistCUDNN
/usr/local/cuda/bin/nvcc <span class="nt">-ccbin</span> g++ <span class="nt">-I</span>/usr/local/cuda/include <span class="nt">-IFreeImage</span>/include  <span class="nt">-m64</span>    <span class="nt">-gencode</span> <span class="nb">arch</span><span class="o">=</span>compute_30,code<span class="o">=</span>sm_30 <span class="nt">-gencode</span> <span class="nb">arch</span><span class="o">=</span>compute_35,code<span class="o">=</span>sm_35 <span class="nt">-gencode</span> <span class="nb">arch</span><span class="o">=</span>compute_50,code<span class="o">=</span>sm_50 <span class="nt">-gencode</span> <span class="nb">arch</span><span class="o">=</span>compute_53,code<span class="o">=</span>sm_53 <span class="nt">-gencode</span> <span class="nb">arch</span><span class="o">=</span>compute_53,code<span class="o">=</span>compute_53 <span class="nt">-o</span> fp16_dev.o <span class="nt">-c</span> fp16_dev.cu
g++ <span class="nt">-I</span>/usr/local/cuda/include <span class="nt">-IFreeImage</span>/include   <span class="nt">-o</span> fp16_emu.o <span class="nt">-c</span> fp16_emu.cpp
g++ <span class="nt">-I</span>/usr/local/cuda/include <span class="nt">-IFreeImage</span>/include   <span class="nt">-o</span> mnistCUDNN.o <span class="nt">-c</span> mnistCUDNN.cpp
/usr/local/cuda/bin/nvcc <span class="nt">-ccbin</span> g++   <span class="nt">-m64</span>      <span class="nt">-gencode</span> <span class="nb">arch</span><span class="o">=</span>compute_30,code<span class="o">=</span>sm_30 <span class="nt">-gencode</span> <span class="nb">arch</span><span class="o">=</span>compute_35,code<span class="o">=</span>sm_35 <span class="nt">-gencode</span> <span class="nb">arch</span><span class="o">=</span>compute_50,code<span class="o">=</span>sm_50 <span class="nt">-gencode</span> <span class="nb">arch</span><span class="o">=</span>compute_53,code<span class="o">=</span>sm_53 <span class="nt">-gencode</span> <span class="nb">arch</span><span class="o">=</span>compute_53,code<span class="o">=</span>compute_53 <span class="nt">-o</span> mnistCUDNN fp16_dev.o fp16_emu.o mnistCUDNN.o  <span class="nt">-LFreeImage</span>/lib/linux/x86_64 <span class="nt">-LFreeImage</span>/lib/linux <span class="nt">-lcudart</span> <span class="nt">-lcublas</span> <span class="nt">-lcudnn</span> <span class="nt">-lfreeimage</span> <span class="nt">-lstdc</span>++ <span class="nt">-lm</span>
iot@iotg-ml-2:~/cudnn_samples_v7/mnistCUDNN<span class="nv">$ </span>./mnistCUDNN 
cudnnGetVersion<span class="o">()</span> : 7104 , CUDNN_VERSION from cudnn.h : 7104 <span class="o">(</span>7.1.4<span class="o">)</span>
Host compiler version : GCC 5.4.0
There are 1 CUDA capable devices on your machine :
device 0 : sms 20  Capabilities 6.1, SmClock 1113.5 Mhz, MemSize <span class="o">(</span>Mb<span class="o">)</span> 7606, MemClock 3003.0 Mhz, <span class="nv">Ecc</span><span class="o">=</span>1, <span class="nv">boardGroupID</span><span class="o">=</span>0
Using device 0
 
Testing single precision
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm ...
Fastest algorithm is Algo 1
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for </span>Algo 0: 0.050176 <span class="nb">time </span>requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for </span>Algo 1: 0.052224 <span class="nb">time </span>requiring 3464 memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for </span>Algo 2: 0.073728 <span class="nb">time </span>requiring 57600 memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for </span>Algo 7: 0.140288 <span class="nb">time </span>requiring 2057744 memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for </span>Algo 5: 0.189440 <span class="nb">time </span>requiring 203008 memory
Resulting weights from Softmax:
0.0000000 0.9999399 0.0000000 0.0000000 0.0000561 0.0000000 0.0000012 0.0000017 0.0000010 0.0000000 
Loading image data/three_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 0.9999288 0.0000000 0.0000711 0.0000000 0.0000000 0.0000000 0.0000000 
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 0.9999820 0.0000154 0.0000000 0.0000012 0.0000006 
 

Result of classification: 1 3 5
 
Test passed!
 
Testing half precision <span class="o">(</span>math <span class="k">in </span>single precision<span class="o">)</span>
Loading image data/one_28x28.pgm
Performing forward propagation ...
Testing cudnnGetConvolutionForwardAlgorithm ...
Fastest algorithm is Algo 1
Testing cudnnFindConvolutionForwardAlgorithm ...
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for </span>Algo 0: 0.041888 <span class="nb">time </span>requiring 0 memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for </span>Algo 1: 0.046080 <span class="nb">time </span>requiring 3464 memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for </span>Algo 2: 0.073728 <span class="nb">time </span>requiring 28800 memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for </span>Algo 7: 0.133088 <span class="nb">time </span>requiring 2057744 memory
^^^^ CUDNN_STATUS_SUCCESS <span class="k">for </span>Algo 5: 0.207904 <span class="nb">time </span>requiring 203008 memory
Resulting weights from Softmax:
0.0000001 1.0000000 0.0000001 0.0000000 0.0000563 0.0000001 0.0000012 0.0000017 0.0000010 0.0000001 
Loading image data/three_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000000 0.0000000 1.0000000 0.0000000 0.0000714 0.0000000 0.0000000 0.0000000 0.0000000 
Loading image data/five_28x28.pgm
Performing forward propagation ...
Resulting weights from Softmax:
0.0000000 0.0000008 0.0000000 0.0000002 0.0000000 1.0000000 0.0000154 0.0000000 0.0000012 0.0000006 
 
Result of classification: 1 3 5
 
Test passed!
iot@iotg-ml-2:~/cudnn_samples_v7/mnistCUDNN<span class="nv">$ </span></code></pre></figure>

<h4 id="step5-install-tensorflow-tensorflow_gpu-1100">Step#5 Install TensorFlow: tensorflow_gpu-1.10.0</h4>

<h6 id="step51-install-bazel-version-0152-first">Step#5.1 install bazel version 0.15.2 first</h6>
<p>https://docs.bazel.build/versions/master/install-ubuntu.html</p>

<p>Using installer binary to install:</p>
<ol>
  <li>sudo apt-get install pkg-config zip g++ zlib1g-dev unzip python</li>
  <li>https://github.com/bazelbuild/bazel/releases:</li>
</ol>

<h6 id="step52-install-nccl">Step#5.2 install NCCL</h6>
<p>https://developer.nvidia.com/nccl/nccl-legacy-downloads</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo </span>apt-key add /var/nccl-repo-2.2.13-ga-cuda9.0/7fa2af80.pub
OK
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">ls</span> /usr/local/cuda/
bin  extras   jre    libnsight  nsightee_plugins  nvvm       samples  src    version.txt
doc  include  lib64  libnvvp    nvml              pkgconfig  share    tools
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">sudo </span>dpkg <span class="nt">-i</span> nccl-repo-ubuntu1604-2.2.13-ga-cuda9.0_1-1_amd64.deb 
<span class="o">(</span>Reading database ... 224455 files and directories currently installed.<span class="o">)</span>
Preparing to unpack nccl-repo-ubuntu1604-2.2.13-ga-cuda9.0_1-1_amd64.deb ...
Unpacking nccl-repo-ubuntu1604-2.2.13-ga-cuda9.0 <span class="o">(</span>1-1<span class="o">)</span> over <span class="o">(</span>1-1<span class="o">)</span> ...
Setting up nccl-repo-ubuntu1604-2.2.13-ga-cuda9.0 <span class="o">(</span>1-1<span class="o">)</span> ...
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span>
 
iot@iotg-ml-2:~/Downloads<span class="nv">$ </span><span class="nb">cd</span> /usr/local/cuda
iot@iotg-ml-2:/usr/local/cuda<span class="nv">$ </span><span class="nb">sudo tar </span>xvf ~/Downloads/nccl_2.2.13-1+cuda9.0_x86_64.txz 
nccl_2.2.13-1+cuda9.0_x86_64/include/
nccl_2.2.13-1+cuda9.0_x86_64/include/nccl.h
nccl_2.2.13-1+cuda9.0_x86_64/lib/
nccl_2.2.13-1+cuda9.0_x86_64/lib/libnccl.so.2.2.13
nccl_2.2.13-1+cuda9.0_x86_64/lib/libnccl_static.a
nccl_2.2.13-1+cuda9.0_x86_64/lib/libnccl.so
nccl_2.2.13-1+cuda9.0_x86_64/lib/libnccl.so.2
nccl_2.2.13-1+cuda9.0_x86_64/COPYRIGHT.txt
nccl_2.2.13-1+cuda9.0_x86_64/NCCL-SLA.txt
iot@iotg-ml-2:/usr/local/cuda<span class="nv">$ </span></code></pre></figure>

<h6 id="step53-buid-tensorflow">Step#5.3 Buid Tensorflow</h6>
<p>https://www.tensorflow.org/install/source#tested_source_configurations</p>

<p>sudo apt-get install git python-dev python-pip
sudo apt-get insall openjdk-8-jdk
pip install -U â€“user pip six numpy wheel mock
pip install -U â€“user keras_applications==1.0.5 â€“no-deps
pip install -U â€“user keras_preprocessing==1.0.3 â€“no-deps</p>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~/<span class="nv">$ </span>git clone https://github.com/tensorflow/tensorflow.git
iot@iotg-ml-2:~/<span class="nv">$ </span><span class="nb">cd </span>tensorflow <span class="o">&amp;&amp;</span> git checkout r1.10

iot@iotg-ml-2:~/tensorflow<span class="nv">$ </span>./configure 
WARNING: Running Bazel server needs to be killed, because the startup options are different.
WARNING: ignoring http_proxy <span class="k">in </span>environment.
WARNING: <span class="nt">--batch</span> mode is deprecated. Please instead explicitly shut down your Bazel server using the <span class="nb">command</span> <span class="s2">"bazel shutdown"</span><span class="nb">.</span>
You have bazel 0.15.0 installed.
Please specify the location of python. <span class="o">[</span>Default is /usr/bin/python]: 
 
 
Found possible Python library paths:
  /usr/local/lib/python2.7/dist-packages
  /usr/lib/python2.7/dist-packages
Please input the desired Python library path to use.  Default is <span class="o">[</span>/usr/local/lib/python2.7/dist-packages]
 
Do you wish to build TensorFlow with jemalloc as malloc support? <span class="o">[</span>Y/n]: y
jemalloc as malloc support will be enabled <span class="k">for </span>TensorFlow.
 
Do you wish to build TensorFlow with Google Cloud Platform support? <span class="o">[</span>Y/n]: y
Google Cloud Platform support will be enabled <span class="k">for </span>TensorFlow.
 
Do you wish to build TensorFlow with Hadoop File System support? <span class="o">[</span>Y/n]: y
Hadoop File System support will be enabled <span class="k">for </span>TensorFlow.
 
Do you wish to build TensorFlow with Amazon AWS Platform support? <span class="o">[</span>Y/n]: y
Amazon AWS Platform support will be enabled <span class="k">for </span>TensorFlow.
 
Do you wish to build TensorFlow with Apache Kafka Platform support? <span class="o">[</span>Y/n]: y
Apache Kafka Platform support will be enabled <span class="k">for </span>TensorFlow.
 
Do you wish to build TensorFlow with XLA JIT support? <span class="o">[</span>y/N]: n
No XLA JIT support will be enabled <span class="k">for </span>TensorFlow.
 
Do you wish to build TensorFlow with GDR support? <span class="o">[</span>y/N]: n
No GDR support will be enabled <span class="k">for </span>TensorFlow.
 
Do you wish to build TensorFlow with VERBS support? <span class="o">[</span>y/N]: n
No VERBS support will be enabled <span class="k">for </span>TensorFlow.
 
Do you wish to build TensorFlow with OpenCL SYCL support? <span class="o">[</span>y/N]: n
No OpenCL SYCL support will be enabled <span class="k">for </span>TensorFlow.
 
Do you wish to build TensorFlow with CUDA support? <span class="o">[</span>y/N]: y
CUDA support will be enabled <span class="k">for </span>TensorFlow.
 
Please specify the CUDA SDK version you want to use. <span class="o">[</span>Leave empty to default to CUDA 9.0]: 9.0
 
 
Please specify the location where CUDA 9.0 toolkit is installed. Refer to README.md <span class="k">for </span>more details. <span class="o">[</span>Default is /usr/local/cuda]: 
 
Please specify the cuDNN version you want to use. <span class="o">[</span>Leave empty to default to cuDNN 7.0]: 7.1
 
Please specify the location where cuDNN 7 library is installed. Refer to README.md <span class="k">for </span>more details. <span class="o">[</span>Default is /usr/local/cuda]:

Do you wish to build TensorFlow with TensorRT support? <span class="o">[</span>y/N]: n
No TensorRT support will be enabled <span class="k">for </span>TensorFlow.

Please specify the NCCL version you want to use. If NCCL 2.2 is not installed, <span class="k">then </span>you can use version 1.3 that can be fetched automatically but it may have worse performance with multiple GPUs. <span class="o">[</span>Default is 2.2]: 

Please specify the location where NCCL 2 library is installed. Refer to README.md <span class="k">for </span>more details. <span class="o">[</span>Default is /usr/local/cuda]:/usr/local/cuda/nccl_2.2.13-1+cuda9.0_x86_64

Please specify a list of comma-separated Cuda compute capabilities you want to build with.
You can find the compute capability of your device at: https://developer.nvidia.com/cuda-gpus.
Please note that each additional compute capability significantly increases your build <span class="nb">time </span>and binary size. <span class="o">[</span>Default is: 6.1]

Do you want to use clang as CUDA compiler? <span class="o">[</span>y/N]: n
nvcc will be used as CUDA compiler.

Please specify which gcc should be used by nvcc as the host compiler. <span class="o">[</span>Default is /usr/bin/gcc]: 

Do you wish to build TensorFlow with MPI support? <span class="o">[</span>y/N]: N
No MPI support will be enabled <span class="k">for </span>TensorFlow.
 
Please specify optimization flags to use during compilation when bazel option <span class="s2">"--config=opt"</span> is specified <span class="o">[</span>Default is <span class="nt">-march</span><span class="o">=</span>native]: 

Would you like to interactively configure ./WORKSPACE <span class="k">for </span>Android builds? <span class="o">[</span>y/N]: N
Not configuring the WORKSPACE <span class="k">for </span>Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding <span class="s2">"--config=&lt;&gt;"</span> to your build command. See tools/bazel.rc <span class="k">for </span>more details.
<span class="nt">--config</span><span class="o">=</span>mkl         	<span class="c"># Build with MKL support.</span>
<span class="nt">--config</span><span class="o">=</span>monolithic  <span class="c"># Config for mostly static monolithic build.</span>
Configuration finished

iot@iotg-ml-2:~/tensorflow<span class="nv">$ </span>
iot@iotg-ml-2:~/tensorflow<span class="nv">$ </span> bazel build <span class="nt">--config</span><span class="o">=</span>opt <span class="nt">--config</span><span class="o">=</span>cuda //tensorflow/tools/pip_package:build_pip_package 
...

Target //tensorflow/tools/pip_package:build_pip_package up-to-date:
  bazel-bin/tensorflow/tools/pip_package/build_pip_package
INFO: Elapsed <span class="nb">time</span>: 233.016s, Critical Path: 179.72s
INFO: 949 processes: 949 local.
INFO: Build completed successfully, 952 total actions
iot@iotg-ml-2:~/tensorflow<span class="nv">$ </span>./bazel-bin/tensorflow/tools/pip_package/build_pip_package /tmp/tensorflow_pkg
Thu Sep 27 17:52:45 PDT 2018 : <span class="o">===</span> Preparing sources <span class="k">in </span><span class="nb">dir</span>: /tmp/tmp.WxMRTJEqdr
~/tensorflow ~/tensorflow
~/tensorflow
Thu Sep 27 17:53:02 PDT 2018 : <span class="o">===</span> Building wheel
warning: no files found matching <span class="s1">'*.dll'</span> under directory <span class="s1">'*'</span>
warning: no files found matching <span class="s1">'*.lib'</span> under directory <span class="s1">'*'</span>
warning: no files found matching <span class="s1">'*.h'</span> under directory <span class="s1">'tensorflow/include/tensorflow'</span>
warning: no files found matching <span class="s1">'*'</span> under directory <span class="s1">'tensorflow/include/Eigen'</span>
warning: no files found matching <span class="s1">'*.h'</span> under directory <span class="s1">'tensorflow/include/google'</span>
warning: no files found matching <span class="s1">'*'</span> under directory <span class="s1">'tensorflow/include/third_party'</span>
warning: no files found matching <span class="s1">'*'</span> under directory <span class="s1">'tensorflow/include/unsupported'</span>
Thu Sep 27 17:53:18 PDT 2018 : <span class="o">===</span> Output wheel file is <span class="k">in</span>: /tmp/tensorflow_pkg
iot@iotg-ml-2:~/tensorflow<span class="nv">$ </span><span class="nb">ls</span> /tmp/tensorflow_pkg
tensorflow-1.10.1-cp27-cp27mu-linux_x86_64.whl

iot@iotg-ml-2:~/tensorflow<span class="nv">$ </span>file /tmp/tensorflow_pkg/tensorflow-1.10.1-cp27-cp27mu-linux_x86_64.whl 
/tmp/tensorflow_pkg/tensorflow-1.10.1-cp27-cp27mu-linux_x86_64.whl: Zip archive data, at least v2.0 to extract
iot@iotg-ml-2:~/tensorflow<span class="nv">$ </span>
<span class="nb">sudo</span> /home/iot/.local/bin/pip <span class="nb">install </span>h5py
<span class="nb">sudo</span> /home/iot/.local/bin/pip <span class="nb">install </span>keras

iot@iotg-ml-2:~/tensorflow<span class="nv">$ </span><span class="nb">sudo</span> /home/iot/.local/bin/pip <span class="nb">install </span>tensorflow-1.10.1-cp27-cp27mu-linux_x86_64.whl </code></pre></figure>

<h3 id="give-a-drive-test">Give a drive test!!!</h3>

<figure class="highlight"><pre><code class="language-bash" data-lang="bash">iot@iotg-ml-2:~<span class="nv">$ </span>python
Python 2.7.12 <span class="o">(</span>default, Dec  4 2017, 14:50:18<span class="o">)</span> 
<span class="o">[</span>GCC 5.4.0 20160609] on linux2
Type <span class="s2">"help"</span>, <span class="s2">"copyright"</span>, <span class="s2">"credits"</span> or <span class="s2">"license"</span> <span class="k">for </span>more information.
<span class="o">&gt;&gt;&gt;</span> import tensorflow as tf
<span class="o">&gt;&gt;&gt;</span> mnist <span class="o">=</span> tf.keras.datasets.mnist
<span class="o">&gt;&gt;&gt;</span> <span class="o">(</span>x_train, y_train<span class="o">)</span>,<span class="o">(</span>x_test, y_test<span class="o">)</span> <span class="o">=</span> mnist.load_data<span class="o">()</span>
Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz
11493376/11490434 <span class="o">[==============================]</span> - 8s 1us/step
11501568/11490434 <span class="o">[==============================]</span> - 8s 1us/step
<span class="o">&gt;&gt;&gt;</span> x_train, x_test <span class="o">=</span> x_train / 255.0, x_test / 255.0
<span class="o">&gt;&gt;&gt;</span> model <span class="o">=</span> tf.keras.models.Sequential<span class="o">([</span>
...   tf.keras.layers.Flatten<span class="o">()</span>,
...   tf.keras.layers.Dense<span class="o">(</span>512, <span class="nv">activation</span><span class="o">=</span>tf.nn.relu<span class="o">)</span>,
...   tf.keras.layers.Dropout<span class="o">(</span>0.2<span class="o">)</span>,
...   tf.keras.layers.Dense<span class="o">(</span>10, <span class="nv">activation</span><span class="o">=</span>tf.nn.softmax<span class="o">)</span>
... <span class="o">])</span>
<span class="o">&gt;&gt;&gt;</span> </code></pre></figure>

<p>TensorFlow dependacy chart:
https://www.tensorflow.org/install/source#tested_source_configurations (at the end of page)</p>

<p><img src="/uploads/ucs/tensorflow-version-deps.jpg" alt="" /></p>


</article>





<section class="rss">
  <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
</section>

<section class="share">
  <span>Share: </span>
  
    
    
    
    
    
    
    
    
  
    
    
    
    
    
      <a href="//www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Flinux%2F2018%2F09%2F08%2FUCS220-M5-ML-platform.html"
        onclick="window.open(this.href, 'linkedin-share', 'width=550,height=255');return false;">
        <i class="fa fa-linkedin-square fa-lg"></i>
      </a>
    
    
    
    
  
</section>




</div>
</div>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h3 class="footer-heading">Wenwei Weng's Blog</h3>

    <div class="site-navigation">

      <p><strong>Site Map</strong></p>
      <ul class="pages">
				
	
	<li class="nav-link"><a href="/about/">About</a>
	

	

	

	
	<li class="nav-link"><a href="/posts/">Posts</a>
	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	

	


      </ul>
    </div>

    <div class="site-contact">

      <p><strong>Contact</strong></p>
      <ul class="social-media-list">
        <li>
          <a href="mailto:weweng@gmail.com">
            <i class="fa fa-envelope-o"></i>
            <span class="username">weweng@gmail.com</span>
          </a>
        </li>

        
          
          <li>
            <a href="https://github.com/weweng" title="Fork me on GitHub">
              <i class="fa fa-github"></i>
              <span class="username">weweng</span>
            </a>
          </li>
          
        
          
          <li>
            <a href="https://www.linkedin.com/in/wenwei-weng/" title="Connect with me on LinkedIn">
              <i class="fa fa-linkedin"></i>
              <span class="username">Wenwei Weng</span>
            </a>
          </li>
          
        

      </ul>
    </div>

    <div class="site-signature">
      <p class="rss-subscribe text"><strong>Subscribe <a href="/feed.xml">via RSS</a></strong></p>
      <p class="text">A simple yet classy theme for your Jekyll based blog.
</p>
    </div>

  </div>

</footer>

<!-- Scripts -->
<script src="//code.jquery.com/jquery-3.4.1.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/lightbox2/2.11.1/js/lightbox.min.js"></script>
<script src="//unpkg.com/popper.js@1"></script>
<script src="//unpkg.com/tippy.js@5"></script>

<script type="text/javascript">
$(document).ready(function() {
  // Default syntax highlighting
  hljs.initHighlightingOnLoad();

  // Header
  var menuToggle = $('#js-mobile-menu').unbind();
  $('#js-navigation-menu').removeClass("show");
  menuToggle.on('click', function(e) {
    e.preventDefault();
    $('#js-navigation-menu').slideToggle(function(){
      if($('#js-navigation-menu').is(':hidden')) {
        $('#js-navigation-menu').removeAttr('style');
      }
    });
  });

	// Enable tooltips via Tippy.js
	if (Array.isArray(window.tooltips)) {
		window.tooltips.forEach(function(tooltip) {
			var selector = tooltip[0];
			var config = tooltip[1];
			tippy(selector, config);
		})
	}
});

</script>






  </body>

</html>
